{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzJtP86NV4QAjxwnIEb83Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beastboy-93/projects/blob/main/language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "import pickle\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {str(e)}\")\n",
        "    exit()\n",
        "\n",
        "# Define language mappings (filename to ISO code)\n",
        "language_mappings = {\n",
        "    \"Hindi.txt\": \"hi\",\n",
        "    \"English.txt\": \"en\",\n",
        "    \"Kannada.txt\": \"kn\",\n",
        "    \"Malayalam.txt\": \"ml\",\n",
        "    \"Odia.txt\": \"or\",\n",
        "    \"Urdu.txt\": \"ur\",\n",
        "    \"Arabic.txt\": \"ar\",\n",
        "    \"Tamil.txt\": \"ta\",\n",
        "    \"Telugu.txt\": \"te\",\n",
        "    \"Bengali.txt\": \"bn\"\n",
        "}\n",
        "\n",
        "# Function to verify dataset integrity\n",
        "def verify_dataset(file_path, lang_code):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "        if not lines:\n",
        "            print(f\"Warning: {file_path} is empty\")\n",
        "            return False\n",
        "        if len(lines) < 1000:\n",
        "            print(f\"Warning: {file_path} has only {len(lines)} sentences (expected ~10,000)\")\n",
        "            return False\n",
        "        # Check script-specific characters\n",
        "        if lang_code == \"ur\" and not any(any(c in line for c in \"ابپتثجچحخدذرزژسشصضطظعغفقکلمنوهیے\") for line in lines[:100]):\n",
        "            print(f\"Warning: {file_path} lacks Urdu characters\")\n",
        "            return False\n",
        "        if lang_code == \"ar\" and not any(any(c in line for c in \"ابتثجحخدذرزسشصضطظعغفقكلمنهوي\") for line in lines[:100]):\n",
        "            print(f\"Warning: {file_path} lacks Arabic characters\")\n",
        "            return False\n",
        "        if lang_code == \"hi\" and not any(any(c in line for c in \"अआइईउऊएऐओऔकखगघचछजझटठडढणतथदधनपफबभमयरलवशषसह\") for line in lines[:100]):\n",
        "            print(f\"Warning: {file_path} lacks Hindi characters\")\n",
        "            return False\n",
        "        print(f\"Verified: {file_path} looks good ({len(lines)} sentences)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying {file_path}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text, lang_code):\n",
        "    # Remove symbols/numbers, keep script-specific characters\n",
        "    valid_chars = (\n",
        "        set(\"abcdefghijklmnopqrstuvwxyz \") if lang_code == \"en\" else\n",
        "        set(\"।\") | set(c for c in text if c.isalpha())\n",
        "    )\n",
        "    text = ''.join(c for c in text if c in valid_chars)\n",
        "\n",
        "    # Tokenize (character-level for deep learning)\n",
        "    if lang_code == \"en\":\n",
        "        tokens = list(text)\n",
        "    else:\n",
        "        try:\n",
        "            tokens = list(text)  # Character-level for Indic/Arabic/Urdu\n",
        "        except:\n",
        "            tokens = list(text)  # Fallback\n",
        "    return ' '.join(tokens).strip()\n",
        "\n",
        "# Combine datasets into a single DataFrame\n",
        "def combine_datasets(input_dir=\"/content/drive/MyDrive/Languages\"):\n",
        "    datasets = []\n",
        "    for filename, lang_code in language_mappings.items():\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {file_path} not found\")\n",
        "            continue\n",
        "        if not verify_dataset(file_path, lang_code):\n",
        "            print(f\"Skipping {file_path} due to verification failure\")\n",
        "            continue\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                texts = [clean_text(line.strip(), lang_code) for line in f if line.strip()]\n",
        "                texts = [t for t in texts if t]  # Remove empty strings\n",
        "                if not texts:\n",
        "                    print(f\"Warning: {file_path} produced no valid texts after cleaning\")\n",
        "                    continue\n",
        "                datasets.append(pd.DataFrame({\"text\": texts, \"language\": lang_code}))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {str(e)}\")\n",
        "            continue\n",
        "    if not datasets:\n",
        "        print(\"Error: No valid datasets found\")\n",
        "        return None\n",
        "    combined = pd.concat(datasets, ignore_index=True)\n",
        "    print(f\"Combined dataset: {len(combined)} sentences\")\n",
        "    return combined\n",
        "\n",
        "# Preprocess data for deep learning\n",
        "def preprocess_data(df, max_len=100, vocab_size=5000):\n",
        "    # Tokenize at character level\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, char_level=True, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(df[\"text\"])\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # Encode labels\n",
        "    label_map = {lang: idx for idx, lang in enumerate(df[\"language\"].unique())}\n",
        "    labels = df[\"language\"].map(label_map).values\n",
        "    labels = to_categorical(labels)\n",
        "\n",
        "    return padded_sequences, labels, tokenizer, label_map\n",
        "\n",
        "# Build CNN model\n",
        "def build_model(vocab_size=5000, max_len=100, num_classes=10):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 128, input_length=max_len),\n",
        "        Conv1D(128, 5, activation=\"relu\"),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate model\n",
        "def train_model(output_model_path=\"/content/drive/MyDrive/best_language_model.keras\"):\n",
        "    # Load and preprocess data\n",
        "    df = combine_datasets()\n",
        "    if df is None:\n",
        "        return None, None, None\n",
        "\n",
        "    X, y, tokenizer, label_map = preprocess_data(df)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(num_classes=y.shape[1])\n",
        "\n",
        "    # Define callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        output_model_path,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        mode=\"max\",\n",
        "        verbose=1\n",
        "    )\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=3,\n",
        "        mode=\"max\",\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[checkpoint, early_stopping]\n",
        "    )\n",
        "\n",
        "    # Save tokenizer and label_map\n",
        "    with open(\"/content/drive/MyDrive/tokenizer.pkl\", \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    with open(\"/content/drive/MyDrive/label_map.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_map, f)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Map back to language codes\n",
        "    reverse_label_map = {idx: lang for lang, idx in label_map.items()}\n",
        "    y_pred_labels = [reverse_label_map[p] for p in y_pred_classes]\n",
        "    y_test_labels = [reverse_label_map[t] for t in y_test_classes]\n",
        "\n",
        "    # Print evaluation\n",
        "    print(f\"Test Accuracy: {accuracy_score(y_test_labels, y_pred_labels):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_labels, y_pred_labels))\n",
        "\n",
        "    return model, tokenizer, label_map\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Train model\n",
        "    model, tokenizer, label_map = train_model()\n",
        "    if model is None:\n",
        "        print(\"Failed to train model\")\n",
        "        exit()\n",
        "\n",
        "    # Test predictions\n",
        "    test_sentences = [\n",
        "        \"سلم صمو ہے۔\",  # Urdu\n",
        "        \"قال سمو هو.\",   # Arabic\n",
        "        \"कगणि समो है।\",  # Hindi\n",
        "        \"salu is bret.\"   # English\n",
        "    ]\n",
        "\n",
        "    # Preprocess test sentences\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    test_padded = pad_sequences(test_sequences, maxlen=100, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(test_padded)\n",
        "    pred_classes = np.argmax(predictions, axis=1)\n",
        "    reverse_label_map = {idx: lang for lang, idx in label_map.items()}\n",
        "    pred_labels = [reverse_label_map[p] for p in pred_classes]\n",
        "\n",
        "    print(\"\\nPredictions:\")\n",
        "    for sentence, pred in zip(test_sentences, pred_labels):\n",
        "        print(f\"Sentence: {sentence} -> Predicted Language: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4PeCHGRRMGO",
        "outputId": "543e6eb5-9f31-48ca-8db5-ccd5b3fafc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Verified: /content/drive/MyDrive/Languages/Hindi.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/English.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Kannada.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Malayalam.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Odia.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Urdu.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Arabic.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Tamil.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Telugu.txt looks good (10000 sentences)\n",
            "Verified: /content/drive/MyDrive/Languages/Bengali.txt looks good (10000 sentences)\n",
            "Combined dataset: 100000 sentences\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1244/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9234 - loss: 0.3034\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/drive/MyDrive/best_language_model.keras\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9237 - loss: 0.3021 - val_accuracy: 1.0000 - val_loss: 5.0407e-07\n",
            "Epoch 2/20\n",
            "\u001b[1m1246/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.7481e-04\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.7458e-04 - val_accuracy: 1.0000 - val_loss: 1.6105e-08\n",
            "Epoch 3/20\n",
            "\u001b[1m1240/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.6115e-05\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.6138e-05 - val_accuracy: 1.0000 - val_loss: 4.6492e-10\n",
            "Epoch 4/20\n",
            "\u001b[1m1239/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.3775e-05\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.3670e-05 - val_accuracy: 1.0000 - val_loss: 7.1526e-11\n",
            "Epoch 4: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Test Accuracy: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ar       1.00      1.00      1.00       984\n",
            "          bn       1.00      1.00      1.00       998\n",
            "          en       1.00      1.00      1.00       983\n",
            "          hi       1.00      1.00      1.00      1021\n",
            "          kn       1.00      1.00      1.00       998\n",
            "          ml       1.00      1.00      1.00      1014\n",
            "          or       1.00      1.00      1.00      1004\n",
            "          ta       1.00      1.00      1.00      1022\n",
            "          te       1.00      1.00      1.00       969\n",
            "          ur       1.00      1.00      1.00      1007\n",
            "\n",
            "    accuracy                           1.00     10000\n",
            "   macro avg       1.00      1.00      1.00     10000\n",
            "weighted avg       1.00      1.00      1.00     10000\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
            "\n",
            "Predictions:\n",
            "Sentence: سلم صمو ہے۔ -> Predicted Language: ur\n",
            "Sentence: قال سمو هو. -> Predicted Language: ar\n",
            "Sentence: कगणि समो है। -> Predicted Language: hi\n",
            "Sentence: salu is bret. -> Predicted Language: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iczcAP_5fBnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}